---
layout: post
title: 딥러닝 여러 optimization 방법 정리
tags: [optimization,adams,sgd,momentum ]
comments: true
---

## gradient descent

Cost Function = 오답을 말하면 벌을 주는 함수 

즉 이 Cost Function 의 값이 0 이 되도록 매개변수들을 최적화시켜주는 방식으로 인공지능은 학습을 한다.  

수학계의 경선식 깨봉박사님 🥰 이 **미분은 상대방 변화는 내 변화의 몇 배 ?** 만 기억하면 된다고 하였다. 

따라서 ∂C/∂w  을 하게 되면 Cost function 의 변화량은 내 변화량 (weight 의 변화량) 의 몇 배 ? 가 된다. 

여기서부터 기하학적으로 접근하는 것이 훨씬 이해가 잘 갔다. 앤드류 응 교수님 방식대로

Cost Function = 1/2m (WX - Y)^2 이라고 한다면 더 풀어서 써보면

Cost Function = 1/2m (X^2**W^2**-2XY**W**+Y^2) 이렇게 **W**(weight) 에 관한 2차식으로 바라볼 수 있다. 

그러면 C(W) (; W를 인자로 하는 Cost Function) = 1/2m (X^2**W^2**-2XY**W**+Y^2) 을 다음과 같이 그릴 수 있다. 물론 실제는 이 W가 일차원이 아니기 때문에 엄청 복잡할 것이다. 하지만 모양은 움푹 들어간 곳이 중간 중간 존재하는 모양으로 나올 것이다. 

![스크린샷 2021-01-26 오전 12 14 39](https://user-images.githubusercontent.com/67775336/105724723-80160c00-5f6b-11eb-8751-f068a0f0f905.png)

즉 ∂C/∂w 는 이 그림에서 보라색 선을 의미하게 된다. 

∂C/∂w 이 양수이면 이렇게 그려질 것이고 

![스크린샷 2021-01-26 오전 12 16 50](https://user-images.githubusercontent.com/67775336/105725013-ce2b0f80-5f6b-11eb-9526-4ff87f75f130.png)



∂C/∂w 이 음수이면 이렇게 그려질 것이다. 

![스크린샷 2021-01-26 오전 12 17 53](https://user-images.githubusercontent.com/67775336/105725161-f61a7300-5f6b-11eb-9b51-4f2514a64a5a.png)



여기서 W = W - [상수] x ∂C/∂w  을 해주게 되면 (gradient descent 의 정의 !)

∂C/∂w 이 양수일 때는 W가 감소하게 되어서 음푹 파인 곳 (cost function 이 낮은 곳) 에 잘 도착한다. 

∂C/∂w 이 음수일 때는 W가 증가하게 되어서 음푹 파인 곳 (cost function 이 낮은 곳) 에 잘 도착한다. 

## local optimum 에 빠지는 문제

![스크린샷 2021-01-26 오전 12 27 40](https://user-images.githubusercontent.com/67775336/105726363-578f1180-5f6d-11eb-849e-68538ecd69f4.png)

실제 C(w) 는 아까 그림처럼 단순하지 않고 feature 수가 늘어날 수록 다차원 공간에서 이렇게 복잡하게 그러질 것이다. 제일 움푹 들어간 곳을 global optimum (궁극적으로 도달하려는 곳) 이라고 한다면 약하게 움푹 들어간 곳 (local optimum) 도 꽤 많이 보인다. 계속 gradient descent 를 진행하다 global optimum 이 아닌 local optimum 에 빠질 지도 모른다. 

## minibatch,sgd optimization 

features 가 n 개가 있고 총 데이터 수 는 m 개인 X (인풋 데이터) 는 m*n 행렬로 나타낼 수 있다. 

![스크린샷 2021-01-26 오전 12 40 13](https://user-images.githubusercontent.com/67775336/105728070-14ce3900-5f6f-11eb-8646-6df726eb72a7.png)



W = W - [상수] x ∂C/∂w  에서 ∂C/∂w 을 풀어서 써주면 

![스크린샷 2021-01-26 오전 12 41 03](https://user-images.githubusercontent.com/67775336/105728174-30394400-5f6f-11eb-8885-28d2c043fd44.png)

이 된다. 즉 기존의 gradient descent 로는 전체 Cost function 의 j번째 feature 의 가중치에 대한 미분값을 위해서는 모든 m개의 j번째 feature(1번 데이터의 j번째 feature + 2번 데이터의 j번째 feature + ... ) 을 곱해주는 과정이 필요하다. 

강아지와 고양이의 사진을 구별하는 binary classification 문제라고 하고, j번째 feature 을  발톱의 날카로운 정도라고 하였을 때 마치 모든 m개의 사진(절반은 고양이,절반은 강아지)을 다 살펴본 후에야 발톱이 ~ 정도 날카로우면 고양이 사진이고, 별로 날카롭지 않으면 강아지 사진이야 라고 해당 feature 의 weight 을 조금씩 학습하는 것이다. (=gradient descent 를 통한 weight update) 



그래서 **minibatch optimization 에서는** 

![스크린샷 2021-01-26 오전 12 48 19](https://user-images.githubusercontent.com/67775336/105729088-34199600-5f70-11eb-8b6e-11d7c9e06ed5.png)

**이 노란색 형광펜 쳐진 부분 (몇 개의 데이터의 해당 j번째 feature 을 봐야지 weight update 가 가능한지) 을 전체 m개에서 지정하고 싶은 mini batch size 로 바꿔준다.** 



즉 전체 m개의 데이터를 모두 보고 j번째 feature 을 학습하는 방식이 아니기 때문에 어떨 때는 weight update 가 Cost function 을 줄여주는 방식으로 매우 잘될 것이다. 하지만 어떨 때는 일부 사진만 보기 때문에 잘못 분석이 될 것이다. 그래서 결국 global optimum 에 도달하지만 global optimum 에 도달하기까지 위 아래로 왔다갔다 (;osciliation 앤드류 응 교수님이 이 단어를 쓰더라구요) 을 더 자주할 것이다. 



sgd 에서는 심지어 일부 데이터만 **노란색 형광펜 쳐진 부분 (몇 개의 데이터의 해당 j번째 feature 을 봐야지 weight update 가 가능한지) 을 전체 m개에서 1개 로 바꿔준다.** 

즉 '발톱의 날카로운 정도' 라는 j번째 feature 을 j번째 weight 업데이트 시 딱 사진 1개만 가지고 학습해주는 것이다. 

sgd 와 mini batch 는 gradient descent 에 비해 전체 m개의 데이터를 모두 도는 시간을 절약할 수 있고 (m의 크기가 크다면 엄청난 시간 !😧) 또 global optimum 에 도달하기까지 시행착오를 더 많이 겪으면서 (위 아래로 왔다갔다, cost function 이 내려가기도 했다가 올라가기도 했다가) local optimum 에 빠져도 더 빨리 빠져나올 수 있다는 장점이 있다. 

하지만 sgd 의 경우 사진 1개 보고 1번째 feature weight update 하고, 또 사진 1개 보고 2번째 feature weight update 하고 이렇게 사진 1개씩만 보기 때문에 한번에 해당 feature 을 알아갈 수 있는 시간이 절대적으로 부족하다. 그래서 gradient descent 나 mini batch 보다 학습하는 데에 더 많은 시간이 걸릴 수 있다 => 목표지점까지 하강하는 스피드가 느려진다. 

## Exponentially weighted averages

exponentially weighted averages 는 이후 나오는 momentum,RMS prop, Adams 에 매우 중요한 개념이다. '지수적으로 / 무게가 취해진 / 평균 ' 이라는 뜻이다. 

정확한 수식의 정의는 이렇다. 

![IMG_55A530CD2530-1](https://user-images.githubusercontent.com/67775336/105790611-fc3e3d00-5fc7-11eb-84b6-68d3ad7b0193.jpeg)

**β 는 0과 1 사이의 숫자이다** 

그리고 각 t시점의 V는 이전 t-1 시점의 V에 연쇄적으로 영향을 받으므로 다음과 같은 규칙이 발견된다. 

![IMG_BD2BA404FD2E-1](https://user-images.githubusercontent.com/67775336/105790859-66ef7880-5fc8-11eb-9497-97445078dfca.jpeg)

어떠한 t 시점의 V에서 θ(t) 를 t시점의 정보라고 하면 오래된 정보일 수록 해당 정보의 가중치가 β배만큼 증가한다. 여기서 β는 1이하의 숫자이므로 β배만큼 가중치가 증가한다는 말은 해당 정보의 비중은 그만큼 작아진다는 것이다. 즉 V(t) 에서 오래된 θ는 β배를 많이 하여서 정보의 비중을 낮추고, 최신 정보의 비중은 높인다. 

이 부분은 자연상수 e 와도 관련이 있다. 

[자연상수 e의 의미](https://angeloyeo.github.io/2019/09/04/natural_number_e.html) 

![스크린샷 2021-01-26 오전 11 24 05](https://user-images.githubusercontent.com/67775336/105791294-22b0a800-5fc9-11eb-8361-b438d5f1452c.png)

위 그림은 (1+<u>0.5</u>)^<u>2</u> 의 그림을 나타낸다. 즉 50%씩 성장하는데 (;1+<u>0.5</u>) 그 성장이 2번인 것이다.  (; ^<u>2</u>)  자연에서는 0.5보다 작은 아주 <u>미세한 단위</u>의 성장이 <u>무한 번</u> 일어난다. 이를 식으로 나타낸 것이 자연 상수이다. 

![IMG_67A2DAC11188-1](https://user-images.githubusercontent.com/67775336/105791599-bd10eb80-5fc9-11eb-81ca-3b9916b272ee.jpeg)



그런데 위 exponentially weighted averages 의 예시에서 (0.9)^n  부분을 아주 미세한 단위의 성장이 아니라 감소를 하는 것으로 해석할 수 있다.  한 번에 0.1씩 감소하는데 (;1-<u>0.1</u>)그것을 총 n번하는 것이다. 

![IMG_0ADC80BB8AE0-1](https://user-images.githubusercontent.com/67775336/105792078-57712f00-5fca-11eb-8541-7375a3f58681.jpeg)

비슷한 원리로 (0.5)^n 도 다음과 같이 해석할 수 있다. 



자연상수 e를 이용해서 다시 정의해보면 이런 식으로 정의할 수 있다.

**자연에서는 작은 아주 <u>미세한 단위</u>의 감소가 <u>무한 번</u> 일어난다면 ?**  (ex.근육의 퇴행)

![IMG_ED046E04B763-1](https://user-images.githubusercontent.com/67775336/105792444-cea6c300-5fca-11eb-8cc5-8c82dd000204.jpeg)

그런데 β 는 0.9 이면 x(;1-β)는 0.1이 되어서 0에 완벽하게 근접한 엄청 엄청 엄청 작은 값까지는 아니다. 따라서 β가 1에 가까우면 가까울 수록 X는 0에 가까워져서  β^(1/1-β) 는 1/e에 가까운 값이 된다. 

β가 0.9 이면 1/1-β 은 10이므로  (0.9)^1 , (0.9)^2, 이렇게 0.9배해줄 때마다 값이 서서히 감소하다가 (0.9)^10 이될 때 1/e 에 엇비슷해진다. 즉 이보다 더 이전인 θ 부터는 너무 쪼그라들어서 비중이 거의 없을 것이다. 

**따라서 exponentially weighted averages 는 1/1-β 시점까지의 서로 다른 가중치를 가진   θ의 평균이라고 할 수 있다.** 

- bias correction

  최초값이 0이어서 생긴 bias 를 최대한 제거해준다. t가 커질 수록 큰 의미는 없지만 초반 V(t) 에서는 bias correction 을 해주는 의미가 있을 수 있다. 

  ![IMG_81AEB053046B-1](https://user-images.githubusercontent.com/67775336/105793613-cc456880-5fcc-11eb-8525-8f4a5fcd877a.jpeg)

  -> [bias correction](https://wikidocs.net/35847)

![스크린샷 2021-01-26 오전 11 51 35](https://user-images.githubusercontent.com/67775336/105793695-eda65480-5fcc-11eb-8c56-174f02ac421f.png)

## Momentum

특히 sgd 의 경우 osciliation(;진동) 이 굉장히 많이 일어난다. (시행착오를 많이 겪는다) 

![스크린샷 2021-01-26 오전 11 57 20](https://user-images.githubusercontent.com/67775336/105794110-acfb0b00-5fcd-11eb-8823-19c6a006b5be.png)

시행착오를 많이 겪을 수록 벡터 W 가 위 아래로 많이 왔다갔다가 한다 (;앤드류 응 교수님 표현에 따르면 진동한다). 이를 보완하고 원하는 바는 위 아래로 많이 왔다갔다 하지 않고 스피디하게 쭉쭉 global mimimum 으로 나아가는 것일 것이다. 

![IMG_EF0E12702B4F-1](https://user-images.githubusercontent.com/67775336/105794419-5b06b500-5fce-11eb-93ae-27a08d9f5ec6.jpeg)

그런데 사실 정확히 말하면 예측 선형식에는 각 feature 의 가중치 W 뿐만 아니라 b (;bias term) 도 있다. bias term 은 average(output) 이어서 어떠한 데이터의 인풋값에 대한 사전 지식이 없을 때 output 은 보통 이 정도로 나올꺼야 라고 미리 꽤 논리적인 추측을 미리 해주는 것이라고 한다. 



그래서 사실 Cost function의 인자는 W 뿐 아니라 b 도 있으므로 gradient descent 에서 Weight도 업데이트시켜줘야 하고 bias term도 업데이트시켜줘야한다. 



![IMG_1A6A4E5E93AF-1](/Users/baekjiyoon/Desktop/IMG_1A6A4E5E93AF-1.jpeg)

Cost function (w,b)

![스크린샷 2021-01-26 오후 12 43 55](https://user-images.githubusercontent.com/67775336/105797857-31e92300-5fd4-11eb-951b-d9104cff0798.png)

**이 때 b 업데이트는 osciliation 위 아래로 얼마나 요동칠지에 영향을 주고 (수직 방향) , w 업데이트는 앞으로 쭉쭉 얼마나 움직일지에 영향을 준다고 한다. (수평 방향)** 이 부분은 왜 그런지 모르겠는데 나중에 stackexchange에 질문을 올려봐야할 것 같다. 직관적으로 이해해보면 w는 기울기니까 나아가는 방향하고 관련이 있고 관련이 있고 b는 절편이니까 수직 방향하고 더 관련이 있는게 아닐까 싶다. 

따라서 

![스크린샷 2021-01-26 오후 12 18 29](https://user-images.githubusercontent.com/67775336/105797995-77a5eb80-5fd4-11eb-8c13-960188d0262c.png)

를 해주게되면 Vdb는 db의 서로 가중치가 다른 평균값을 의미하므로 위로 움직였다가 (+) 아래로 움직이면서 (-) 상쇄되면서 0에 가까운 값이 된다. 

VdW 는 dw의 서로 가중치가 다른 평균값을 의미하는데 dw는 수평방향으로 조금씩 나아가므로 점점 값이 커진다. 

![스크린샷 2021-01-26 오후 12 18 34](https://user-images.githubusercontent.com/67775336/105798151-d3707480-5fd4-11eb-8283-72bd2644a1b9.png)

따라서 각각 VdW,Vdb 를 통해 w,b 를 업데이트해주게되면 수직방향으로 왔다갔다하는 것은 b 값을 적게 업데이트해주므로 조정되고 수평방향으로는 w값을 많이 업데이트해주므로 더 스피디하게 나아갈 수 있다. 

## RMSprop

![스크린샷 2021-01-26 오후 1 00 46](https://user-images.githubusercontent.com/67775336/105798891-88eff780-5fd6-11eb-92f4-5a35ba7d2dc3.png)





![스크린샷 2021-01-26 오후 12 59 40](https://user-images.githubusercontent.com/67775336/105798926-9f964e80-5fd6-11eb-83bc-490309bb2aa0.png)

RMS prop 에서 VdW 는 dW^2 (;element wise) 의 서로 가중치가 다른 평균값을 의미하므로 sgd 가 수평 방향으로 아장아장 나아간다고 하면 상대적으로 작은 값이다. 

Vdb는 db^2(;element wise) 의 서로 가중치가 다른 평균값을 의미하므로 sgd가 수직방향으로 위 아래 왔다갔다 하지만 부호를 빼준다면(제곱하므로) 값이 상쇄되지 않고 수직방향으로는 더욱 성큼성큼 움직이므로 상대적으로 큰 값이다. 

따라서 W 업데이트 시 구 W 에서 분모에 작은 값이 들어간 수를 빼주므로 (;분모가 작으므로 상대적으로 큰 수) 수평방향으로는 많이 업데이트된다. b 업데이트 시 구b 에서 분모에 큰 값이 들어간 수를 빼주므로(;분모가 크므로 상대적으로 작은 수) 수직 방향으로는 조금 업데이트된다. 

그래서 위아래로 진동은 적게하면서 앞으로는 더 스피디하게 나아갈 수 있게 된다. 

## adam (momentum + rms prop)

![IMG_3EF827A5FC93-1](https://user-images.githubusercontent.com/67775336/105799353-ab364500-5fd7-11eb-83eb-626c7da0795b.jpeg)

이다 ! 

## 이렇게 매우 직관적으로나마

그동안 햇갈렸던 개념을 조금 정리해보았다 !.! 