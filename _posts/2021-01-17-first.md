---
layout: post
title: LSTM,Bi-LSTM,attentionì˜ ê°œë…ê³¼ ë‹¨ì–´ nmt ëª¨ë¸ (ì½”ë“œ)
subtitle: basic nmt
tags: [lstm,bi-lstm,attention,nmt]
comments: true
---

## Intro

ì•ˆë…•í•˜ì„¸ìš”, nlp ì˜ ì£¼ ë¶„ì•¼ì—¬ì„œ ê·¸ëŸ°ê°€ ê¸°ê³„ë²ˆì—­ì€ í‰ì†Œ í•­ìƒ ê³µë¶€í•˜ê³  ì‹¶ë˜ ì˜ì—­ ê°€ìš´ë° í•˜ë‚˜ì˜€ìŠµë‹ˆë‹¤. ì „ê³µìœ¼ë¡œ í•  ì •ë„ì˜ ì‹¬ë„ ìˆëŠ” ì´í•´ëŠ” ë¬¼ë¡  í•  ìˆ˜ ì—†ê² ì§€ë§Œ ì–´ëŠ ì •ë„ ìœ ëª…í•œ ë…¼ë¬¸ ìœ„ì£¼ì˜ íŠ¸ë Œë“œì™€ ì½”ë“œ ì´í•´ëŠ” ë”°ë¼ì¡ê³  ì‹¶ìŠµë‹ˆë‹¤. + ê·¸ë¦¬ê³  ì €ëŠ” í˜„ì¬ nmt ëŒ€íšŒ ì°¸ê°€ë¥¼ ì¤€ë¹„í•˜ê³  ìˆê¸° ë•Œë¬¸ì´ê¸°ë„ í•©ë‹ˆë‹¤ ğŸ‘©â€ğŸ’»)

- LSTM ê°œë…, LSTM nmt ëª¨ë¸ ì½”ë“œ 

- biLSTM + attention

- transformer ê°œë… + open nmt íŠœí† ë¦¬ì–¼

- nmtì— ìˆì–´ì„œ low resources í•´ê²° ë¬¸ì œ

  - back translation

  - êµµì§êµµì§í•œ ë…¼ë¬¸ ì°¾ì•„ì„œ ê³µë¶€í•˜ê¸°

  - => ì–´ë–»ê²Œ ì´ë²ˆ ëŒ€íšŒì—ì„œ í•™ìŠµì´ ì˜ ë˜ê¸° ìœ„í•œ ë°ì´í„°ë¥¼ êµ¬ì¶•í• ê¹Œ ? ì¸ì‚¬ì´íŠ¸ ì–»ê¸° ! (ì œê°€ ë§¡ì€ ë¶€ë¶„ì…ë‹ˆë‹¤. ì˜ í•  ìˆ˜ ìˆê² ì£  ? ğŸ¥º)                 

    

## lstm ê°œë…

**ê·¸ëŸ¼ ì§€ê¸ˆë¶€í„° ì—´ì‹¬íˆ ê³µë¶€í•´ë³´ê² ìŠµë‹ˆë‹·** ğŸ¤œ ê·¸ê°„ ì €ëŠ” lstm ì„ ì´í•´í•˜ê³ ì ê°–ì€ ë…¸ë ¥ì„ í•˜ì˜€ìŠµë‹ˆë‹¤. ì €ì—ê²Œ ì œì¼ ì¢‹ì•˜ë˜ ë°©ë²•ì€ <u>ë³€ìˆ˜ ëª… ì •ì˜ë¥¼ ì •í™•í•˜ê²Œ ì •ë¦¬í•´ë³´ê³  ê·¸ëƒ¥ ì˜ ì •ë¦¬ëœ ê·¸ë¦¼ í•œ ë°©ì— ì´í•´í•˜ëŠ” ê²ƒ</u>ì´ì—ˆìŠµë‹ˆë‹¤. 

í•µì‹¬ : rnn ì˜ ì€ë‹‰ì¶©ì¸ ë‹¨ìˆœ h ë§ê³  C ë¡œ íŒë‹¨í•˜ì

- C : ê¸°ì–µ/ë§ê° ì •ë„ë¥¼ í•™ìŠµí•œ ì‰˜
- gate : ì‹ í˜¸ì˜ ì–‘ì„ ì¡°ì •í•´ì£¼ëŠ” ê¸°ë²•
  - forget gate : ê·¸ ì „ ì‹ í˜¸ì—ì„œ ë¶ˆí•„ìš”í•œ ì •ë³´ ë§ê° "ì˜ì–´ ë¬¸ì¥ì—ì„œ Snoopy is happy. Children are happy " ë¼ëŠ” ë¬¸ì¥ì´ ìˆë‹¤ê³  í•˜ì˜€ì„ ë•Œ "ì•ì— ì£¼ì–´ëŠ” 3ì¸ì¹­ì¸ë° is ì•¼" ë¥¼ ê¸°ì–µí•˜ëŠ” ê²ƒì€ ë¶ˆí•„ìš”í•©ë‹ˆë‹¤. ì£¼ì–´ê°€ ë°”ë€Œì—ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì´ì²˜ëŸ¼ ê° ì‹ í˜¸ì—ì„œ ë” ì´ìƒ ì“¸ëª¨ ì—†ëŠ” ë¶€ë¶„ì´ ëœ ê²ƒì€ ìŠì–´ì¤˜ì•¼ í•©ë‹ˆë‹¤. 
  - input gate (=update gate) : ë°”ë€ ì •ë³´ë¥¼ ìƒˆë¡œ ì—…ë°ì´íŠ¸ í•´ì£¼ê¸° "ì£¼ì–´ê°€ ë°”ë€Œì—ˆì–´. ì´ì œ ë” ì´ìƒ ì£¼ì–´ê°€ 3ì¸ì¹­ì´ ì•„ë‹ˆì•¼"
  - output gate : ì–¼ë§ˆë‚˜ ë°–ì— í‘œì¶œí• ì§€ ê²°ì •.



ê·¸ë¦¬ê³  ê° ê³¼ì •ì´ ì˜ ì •ë¦¬ëœ ê·¸ë¦¼ì…ë‹ˆë‹¤. 

![IMG_D138D8F704F5-1](https://user-images.githubusercontent.com/67775336/104831877-f366b080-58cf-11eb-8238-38180d36ead2.jpeg)

ë¬¼ê²° í‘œì‹œëŠ” ì˜ˆë¹„ C ë¼ê³  ë³´ì‹œë©´ ë©ë‹ˆë‹¤. (ì•„ì§ CëŠ” ì•„ë‹Œ ì˜ˆë¹„ C)

Cê°€ ì•„ë‹Œ ì˜ˆë¹„ Cì¸ ì´ìœ ëŠ” t ì‹œì ì— ì´ ë“¤ì–´ì˜¨ ì‹ í˜¸(ì˜ˆë¹„ C)ì—ì„œ input gate ë¥¼ ê±°ì³ì„œ ìƒˆë¡œ ì—…ë°ì´íŠ¸í•´ì¤˜ì•¼í•˜ëŠ” ë¶€ë¶„ë§Œ ë‚¨ê²¨ì•¼í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. (C)

ì˜ˆë¹„ C(t) : tanh(W x h(t-1) + W(t) x X(t) + b(t) ) 

i(t)  (input gate ë¥¼ ê±°ì³ì„œ ë‚˜ì˜¨ ê°’)  : sigmoid(W(i) x (h(t-1) + X(t) ) + b(i) )

f(t) (forget gate ë¥¼ ê±°ì³ì„œ ë‚˜ì˜¨ ê°’) : sigmoid(W(f) x (h(t-1) + X(t)) + b(f))

C(t) : f(t) x C(t-1) + i(t) x ì˜ˆë¹„ C(t)

O(t) : sigmoid(W(o) x (h(t-1) + X(t) + b(o))

h(t),y(t) : O(t) x tanh(C(t)) 

ê° ì‹œì ë§ˆë‹¤ì˜ ëº„ êº¼ ë‹¤ ë¹¼ê³ ,ìµœì¢… ê²°ì •ë˜ì–´ì„œ ë‚˜ì˜¨ output ì„ ê·¸ë•Œê·¸ë•Œ ë‹¤ ì €ì¥í•´ë†“ê¸° ìœ„í•´ì„œ hidden shell ì—ëŠ” t ì‹œì ì˜ ìµœì¢… ê²°ê³¼ì™€ ë™ì¼í•œ ë²¡í„°ê°€ ì €ì¥ë©ë‹ˆë‹¤. 



C (x) ; x ì‹œì ì—ì„œì˜ C ê°’ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. 

## lstm nmt ëª¨ë¸ ì½”ë“œ ([Word-Level ë²ˆì—­ê¸° ë§Œë“¤ê¸°](https://wikidocs.net/86900))

ìœ„í‚¤ë…ìŠ¤ í•´ë‹¹ ë§í¬ì˜ ì½”ë“œë¥¼ ì œê°€ ë³µìŠµí•œ ë¶€ë¶„ì…ë‹ˆë‹¤. ì €ëŠ” í…ì„œí”Œë¡œìš°ì˜ ë§¤ìš° ìœ ì—°í•˜ì§€ ì•Šì€ ìˆœì°¨í˜• APIë§Œ ì¡°ê¸ˆ ë§Œì§€ì‘ê±°ë¦° ì •ë„ì´ê¸° ë•Œë¬¸ì— ì°¨ê·¼ì°¨ê·¼ ì´í•´ê°€ í•„ìš”í–ˆê³ , ê·¸ ê³¼ì •ì„ ê³µìœ í•´ë³¼ê¹Œ í•©ë‹ˆë‹¤. 

ìš°ì„  í•´ë‹¹ ë§í¬ì˜ ì½”ë“œì— ë‚˜ì™€ìˆëŠ” ë°ì´í„° ì „ì²˜ë¦¬ ë‹¨ê³„ëŠ” ìƒëµí•˜ê² ìŠµë‹ˆë‹¤. ì˜ ì „ì²˜ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ê°€ ë‚˜ì˜µë‹ˆë‹¤. 

```python
sents_en_in, sents_fra_in, sents_fra_out = load_preprocessed_data()
print(sents_en_in[:5])
print(sents_fra_in[:5])
print(sents_fra_out[:5])
```

```python
[['go', '.'], ['hi', '.'], ['hi', '.'], ['run', '!'], ['run', '!']]
[['<sos>', 'va', '!'], ['<sos>', 'salut', '!'], ['<sos>', 'salut', '.'], ['<sos>', 'cours', '!'], ['<sos>', 'courez', '!']]
[['va', '!', '<eos>'], ['salut', '!', '<eos>'], ['salut', '.', '<eos>'], ['cours', '!', '<eos>'], ['courez', '!', '<eos>']]
```

ì˜ì–´ -> í”„ë‘ìŠ¤ì–´ nmt ì´ê¸° ë•Œë¬¸ì— sents_en_in ì€ ê° ë¬¸ì¥ì„ ë‹¨ì–´ í† í°ìœ¼ë¡œ ìª¼ê°œì„œ array ì— ë„£ì–´ì¤€ ê²ƒì˜ ë‚˜ì—´ì´ê³   sents_fra_in / out ì˜ ê²½ìš° lstm ë””ì½”ë” ëª¨ë¸ì„ í™œìš©í•´ì„œ ì• í† í°ë“¤ì„ ë³´ê³  í•´ë‹¹ í† í°ì„ ë§ì¶”ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•  ê²ƒì´ê¸° ë•Œë¬¸ì— ìœ„ì™€ ê°™ì´ ì „ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.

-> í† í¬ë‚˜ì´ì§• -> íŒ¨ë”©ì„ í•´ì„œ ì„±ê³µì ìœ¼ë¡œ ì¸ì½”ë”©í•´ì¤ë‹ˆë‹¤. (ê¸°ì¡´ nlpì™€ ë™ì¼)

ê·¸ë¦¬ê³  í•´ë‹¹ ì¸ì½”ë”© ê°’ì´ ì–´ë–¤ ìì—°ì–´ í† í°ì¸ì§€, í•´ë‹¹ ìì—°ì–´ í† í°ì´ ì–´ë– í•œ í† í¬ë‚˜ì´ì € ì¸ì½”ë”© ê°’ìœ¼ë¡œ ë³€í™˜ë˜ì—ˆëŠ”ì§€ë¥¼ ê°ê° ì €ì¥í•´ì¤ë‹ˆë‹¤. ê²°êµ­ nmt ëª¨ë¸ì€ ì˜ì–´ ë¬¸ì¥ì„ ì¸ì½”ë”ì— ë„£ìœ¼ë©´ ë””ì½”ë” ì•„ì›ƒí’‹ìœ¼ë¡œ í”„ë‘ìŠ¤ì–´ì— í•´ë‹¹í•˜ëŠ” ìˆ«ìë¥¼ ë°˜í™˜í•  ê²ƒì¸ë° ì´ ìˆ«ìë¥¼ ë‹¤ì‹œ ìì—°ì–´ í† í°ìœ¼ë¡œ ë³€í™˜í•´ì•¼ì§€ë§Œ ë¹„ë¡œì†Œ ìš°ë¦¬ê°€ ì•Œì•„ë³¼ ìˆ˜ ìˆëŠ” ì˜ì–´ -> í”„ë‘ìŠ¤ì–´ ë²ˆì—­ì´ ì™„ì„±ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. 

```python
src_to_index = tokenizer_en.word_index
index_to_src = tokenizer_en.index_word # í›ˆë ¨ í›„ ê²°ê³¼ ë¹„êµí•  ë•Œ ì‚¬ìš©

tar_to_index = tokenizer_fra.word_index # í›ˆë ¨ í›„ ì˜ˆì¸¡ ê³¼ì •ì—ì„œ ì‚¬ìš©
index_to_tar = tokenizer_fra.index_word # í›ˆë ¨ í›„ ê²°ê³¼ ë¹„êµí•  ë•Œ ì‚¬ìš©
```



**ì£¼ì˜í•  ì **

ì œê°€ ì½”ë“œë¥¼ ë¦¬ë·°í•˜ë©´ì„œ ë¬´ì²™ì´ë‚˜ í–‡ê°ˆë ¸ë˜ ë¶€ë¶„ë“¤ì…ë‹ˆë‹¤.

- return_sequences=True ë¡œ ì„¤ì •í•œ ê²½ìš°, ê° t ì‹œì ì˜ output ì„ ì¶œë ¥í•©ë‹ˆë‹¤. many to many ë“±ë“±ì—ì„œ ì“°ëŠ” ì„¤ì •ì…ë‹ˆë‹¤.

- return_states=True ë¡œ ì„¤ì •í•œ ê²½ìš°, ê° t ì‹œì ì˜  hidden state ê¹Œì§€ ê°™ì´ ì¶œë ¥ë©ë‹ˆë‹¤. hidden state ëŠ” ë°–ì— í‘œì¶œë˜ëŠ” y(t) ì´ì™¸ì— ë¸”ë™ë°•ìŠ¤ì— ìˆ¨ê²¨ì§„ ìƒíƒœì´ë¯€ë¡œ lstm ëª¨ë¸ì—ì„œëŠ” C(t),h(t) ê°€ ë  ê²ƒì…ë‹ˆë‹¤. 

  ![IMG_C650C618461A-1](https://user-images.githubusercontent.com/67775336/104832448-75f16f00-58d4-11eb-88fc-61c88d475f4c.jpeg)

- units != timesteps != hidden cells ì…ë‹ˆë‹¤. 

  ê° timestep ë§ˆë‹¤ hidden cells ê°€ ìˆê³  hidden cells ëŠ” hidden units ë¡œ êµ¬ì„±ë˜ì–´ìˆìŠµë‹ˆë‹¤. 

  ![IMG_049493464B6D-1](https://user-images.githubusercontent.com/67775336/104832493-d1236180-58d4-11eb-9668-e861d71e419e.jpeg)

  ì¦‰ hidden cell ì€ ê° ë‹¨ì–´ë¥¼ ëª‡ ì°¨ì›ìœ¼ë¡œ ì¶œë ¥í• ì§€ dimension ì„ ì˜ë¯¸í•˜ê²Œ ë©ë‹ˆë‹¤.

  ê° ì¼€ë¼ìŠ¤ ë ˆì´ì–´ì— hidden cell ì„ LSTM (30) ì´ëŸ° ì‹ìœ¼ë¡œ ì§€ì •í•˜ë¯€ë¡œ ì´ ë•Œ ê° ë‹¨ì–´ëŠ” 30ì°¨ì› output ê°’ìœ¼ë¡œ ì¶œë ¥ë  ê²ƒì…ë‹ˆë‹¤. 

  

  **ì¸ì½”ë”,ë””ì½”ë” ì„¤ê³„**

  ```python
  latent_dim = 50 #í•œ ë‹¨ì–´ë‹¹ ì°¨ì› ìˆ˜ 
  # ì¸ì½”ë”
  encoder_inputs = Input(shape=(None,))
  enc_emb =  Embedding(src_vocab_size, latent_dim)(encoder_inputs) # ì„ë² ë”© ì¸µ
  enc_masking = Masking(mask_value=0.0)(enc_emb) # íŒ¨ë”© 0ì€ ì—°ì‚°ì—ì„œ ì œì™¸
  encoder_lstm = LSTM(latent_dim, return_state=True) # ìƒíƒœê°’ ë¦¬í„´ì„ ìœ„í•´ return_stateëŠ” True
  encoder_outputs, state_h, state_c = encoder_lstm(enc_masking) # ì€ë‹‰ ìƒíƒœì™€ ì…€ ìƒíƒœë¥¼ ë¦¬í„´
  encoder_states = [state_h, state_c] # ì¸ì½”ë”ì˜ ì€ë‹‰ ìƒíƒœì™€ ì…€ ìƒíƒœë¥¼ ì €ì¥
  ì´ì œ ë””ì½”ë”ë¥¼ ì„¤ê³„í•©ë‹ˆë‹¤.
  ```

   

encoder_outputs, state_h, state_c = encoder_lstm(enc_masking) # ì€ë‹‰ ìƒíƒœì™€ ì…€ ìƒíƒœë¥¼ ë¦¬í„´

ì¸ ì´ìœ ëŠ” return_state=True ì´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì¦‰ ê° ì‹œì ë§ˆë‹¤ ì´í›„ ì‹œì ìœ¼ë¡œ ì „ë‹¬ë˜ëŠ” (ì´ëŸ° ì •ë³´ê°€ t-1ì‹œì ê¹Œì§€ì— ìˆì—ˆì–´) H,C ê°’ë„ ê°™ì´ ë°˜í™˜ë˜ê²Œ ë©ë‹ˆë‹¤. 

ì €ëŠ” H(t),C(t) hidden stateë¥¼ ë“œë¼ë§ˆ ì´ì „ í™” summary ì •ë„ë¡œ ì§ê´€ì ìœ¼ë¡œ ì´í•´í–ˆìŠµë‹ˆë‹¤.  ì œê°€ ë„ˆë¬´ ë°”ì  ë•ŒëŠ” ë“œë¼ë§ˆ ì´ì „ í™”ë“¤ì€ ê±´ë„ˆë›°ê³  ë³¼ ë•Œê°€ ë§ì€ë° ì´ ë•Œ ì´ì „ í™”ì—ì„œ ë¬´ìŠ¨ ë¬´ìŠ¨ ì¼ì´ ìˆì—ˆëŠ”ì§€ ë¹¨ë¦¬ê°ê¸°ë¡œ ìš”ì•½í•´ì„œ ë³´ì—¬ì£¼ëŠ” ê³¼ì •ì´ ì•„ë‹ê¹Œ ì‹¶ìŠµë‹ˆë‹¤. 

ë‹¤ìŒì€ ë””ì½”ë”ì…ë‹ˆë‹¤. 

```python
# ë””ì½”ë”
decoder_inputs = Input(shape=(None,))
dec_emb_layer = Embedding(tar_vocab_size, latent_dim) # ì„ë² ë”© ì¸µ
dec_emb = dec_emb_layer(decoder_inputs) # íŒ¨ë”© 0ì€ ì—°ì‚°ì—ì„œ ì œì™¸
dec_masking = Masking(mask_value=0.0)(dec_emb)

# ìƒíƒœê°’ ë¦¬í„´ì„ ìœ„í•´ return_stateëŠ” True, ëª¨ë“  ì‹œì ì— ëŒ€í•´ì„œ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ return_sequencesëŠ” True
decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) 

# ì¸ì½”ë”ì˜ ì€ë‹‰ ìƒíƒœë¥¼ ì´ˆê¸° ì€ë‹‰ ìƒíƒœ(initial_state)ë¡œ ì‚¬ìš©
decoder_outputs, _, _ = decoder_lstm(dec_masking,
                                     initial_state=encoder_states)

# ëª¨ë“  ì‹œì ì˜ ê²°ê³¼ì— ëŒ€í•´ì„œ ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œ ì¶œë ¥ì¸µì„ í†µí•´ ë‹¨ì–´ ì˜ˆì¸¡
decoder_dense = Dense(tar_vocab_size, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)
```

![IMG_518F9C1948B0-1](https://user-images.githubusercontent.com/67775336/104832697-3af03b00-58d6-11eb-819b-991c2850b12d.jpeg)

ë””ì½”ë” ì½”ë“œë¥¼ ë³´ì‹œë©´ decoder_outputs, _, _ = decoder_lstm(dec_masking,
                                     initial_state=encoder_states) ì…ë‹ˆë‹¤. 



ì¦‰ ì¸ì½”ë” ëª¨ë¸ì´ ëŒë©´ì„œ ê° ì‹œì ë§ˆë‹¤ hidden state (h,c)ë¥¼ ì €ì¥í•˜ê²Œ ë˜ê³  ì „ë‹¬ë˜ê³  ì „ë‹¬ë˜ë‹¤ê°€ ë””ì½”ë”ì—ê¹Œì§€ ë„˜ì–´ê°‘ë‹ˆë‹¤. ì¸ì½”ë” ë¬¸ì¥ì˜ ì „ì²´ ë¬¸ë§¥ì„ ë‹´ì€ ë²¡í„°ë¼ê³  í•˜ì—¬ context vector ì´ë¼ê³  ë¶€ë¥´ê¸°ë„ í•©ë‹ˆë‹¤. 

**ì§œë†“ì€ ëª¨ë¸ì— í•™ìŠµ ë°ì´í„°,ì •ë‹µê°’ ë„£ì–´ì£¼ê¸°**

```python
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
```

ì €ëŠ” ì²˜ìŒì— sequential api ë¡œ ìƒê°í–ˆê¸° ë•Œë¬¸ì— ì¸í’‹ê°’ì„ ë‘ ê°œë¥¼ ë„£ëŠ” ê²ƒì´ ë„ˆë¬´ ì´í•´ê°€ ê°€ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì•Œê³ ë³´ë‹ˆ functional api ëŠ” ìˆœì°¨í˜•ê³¼ëŠ” ë‹¬ë¦¬ ë§¤ìš° ìœ ì—°í•˜ê²Œ ì—¬ëŸ¬ ê°œì˜ nput ì„ ë°›ì•„ì„œ í•™ìŠµí•  ìˆ˜ ìˆëŠ” êµ¬ì¡°ë¼ëŠ” ê²ƒì„ ì•Œê²Œë˜ì—ˆìŠµë‹ˆë‹¤. âœ”ï¸

![IMG_31256FE5BBAF-1](https://user-images.githubusercontent.com/67775336/104832787-21032800-58d7-11eb-9728-5917ed93ee64.jpeg)



ê·¸ë¦¬ê³  ì½”ë“œì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •í•´ì„œ í•™ìŠµí•˜ëŠ” ê³¼ì •ì€ ê¸°ì¡´ nlp task ì™€ ë™ì¼í•˜ê²Œ ì§„í–‰ë˜ì–´ì„œ ìƒëµí•˜ì˜€ìŠµë‹ˆë‹¤. 

**test ê³¼ì •**

ê·¸ëŸ¬ë©´ ì´ì œ ì•„ë¬´ ì˜ì–´ ë¬¸ì¥ì´ë‚˜ ë„£ì–´ì„œ ë‚˜ì˜¤ëŠ” í”„ë‘ìŠ¤ì–´ ë””ì½”ë” ì•„ì›ƒí’‹ì„ í†µí•´ì„œ ëª¨ë¸ ì„±ëŠ¥ì„ ê°€ëŠ í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

**í…ŒìŠ¤íŠ¸ìš© ì¸ì½”ë”,ë””ì½”ë” ëª¨ë¸** 

```python
# ì¸ì½”ë”
encoder_model = Model(encoder_inputs, encoder_states)

# ë””ì½”ë”
# ì´ì „ ì‹œì ì˜ ìƒíƒœë¥¼ ë³´ê´€í•  í…ì„œ
decoder_state_input_h = Input(shape=(latent_dim,))
decoder_state_input_c = Input(shape=(latent_dim,))
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]

# í›ˆë ¨ ë•Œ ì‚¬ìš©í–ˆë˜ ì„ë² ë”© ì¸µì„ ì¬ì‚¬ìš©
dec_emb2= dec_emb_layer(decoder_inputs)

# ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ì„ ìœ„í•´ ì´ì „ ì‹œì ì˜ ìƒíƒœë¥¼ í˜„ ì‹œì ì˜ ì´ˆê¸° ìƒíƒœë¡œ ì‚¬ìš©
decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)
decoder_states2 = [state_h2, state_c2]

# ëª¨ë“  ì‹œì ì— ëŒ€í•´ì„œ ë‹¨ì–´ ì˜ˆì¸¡
decoder_outputs2 = decoder_dense(decoder_outputs2)

decoder_model = Model(
    [decoder_inputs] + decoder_states_inputs,
    [decoder_outputs2] + decoder_states2)
```

**decode_sequence í•¨ìˆ˜**

```python
def decode_sequence(input_seq):
    # ì…ë ¥ìœ¼ë¡œë¶€í„° ì¸ì½”ë”ì˜ ìƒíƒœë¥¼ ì–»ìŒ
    states_value = encoder_model.predict(input_seq)

    # <SOS>ì— í•´ë‹¹í•˜ëŠ” ì •ìˆ˜ ìƒì„±
    target_seq = np.zeros((1,1))
    target_seq[0, 0] = tar_to_index['<sos>']

    stop_condition = False
    decoded_sentence = ''

    # stop_conditionì´ Trueê°€ ë  ë•Œê¹Œì§€ ë£¨í”„ ë°˜ë³µ
    # êµ¬í˜„ì˜ ê°„ì†Œí™”ë¥¼ ìœ„í•´ì„œ ì´ í•¨ìˆ˜ëŠ” ë°°ì¹˜ í¬ê¸°ë¥¼ 1ë¡œ ê°€ì •í•©ë‹ˆë‹¤.
    while not stop_condition:
        # ì´ì  ì‹œì ì˜ ìƒíƒœ states_valueë¥¼ í˜„ ì‹œì ì˜ ì´ˆê¸° ìƒíƒœë¡œ ì‚¬ìš©
        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)

        # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë‹¨ì–´ë¡œ ë³€í™˜
        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        sampled_char = index_to_tar[sampled_token_index]

         # í˜„ì¬ ì‹œì ì˜ ì˜ˆì¸¡ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡ ë¬¸ì¥ì— ì¶”ê°€
        decoded_sentence += ' '+sampled_char

        # <eos>ì— ë„ë‹¬í•˜ê±°ë‚˜ ì •í•´ì§„ ê¸¸ì´ë¥¼ ë„˜ìœ¼ë©´ ì¤‘ë‹¨.
        if (sampled_char == '<eos>' or
           len(decoded_sentence) > 50):
            stop_condition = True

        # í˜„ì¬ ì‹œì ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë‹¤ìŒ ì‹œì ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì €ì¥
        target_seq = np.zeros((1,1))
        target_seq[0, 0] = sampled_token_index

        # í˜„ì¬ ì‹œì ì˜ ìƒíƒœë¥¼ ë‹¤ìŒ ì‹œì ì˜ ìƒíƒœë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì €ì¥
        states_value = [h, c]

    return decoded_sentence
```

í•˜ë‚˜í•˜ë‚˜ ëœ¯ì–´ì„œ ë³´ë©´

 output_tokens, h, c = decoder_model.predict([target_seq] + states_value) ì´ê³  

ì•ì„œ ì„¤ê³„í•œ ëª¨ë¸ì€  input ê°’ ë‘ ê°œ (decoder_input,decoder_states_inputs) ê°€ ë“¤ì–´ê°„ë‹¤ê³  ì •ì˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

decoder_model = Model(
    **[decoder_inputs] + decoder_states_inputs**,
    [decoder_outputs2] + decoder_states2) 



ê·¸ëŸ°ë° decoder_states_inputs ìë¦¬ì— ë„£ì–´ì¤€ states_value ì˜ ê²½ìš° states_value = encoder_model.predict(input_seq) ì…ë‹ˆë‹¤. ê·¸ë¦¬ê³  encoder_model ì€ encoder_model = Model(encoder_inputs, **encoder_states**) ì´ê¸° ë•Œë¬¸ì— encoder_states ì¸ ì­‰ì­‰ ì „ë‹¬ë˜ëŠ” h(t),h(t-1),... ,c(t),c(t-1),... ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. ì¦‰ ë””ì½”ë” ì²« ë‹¨ì–´(í† í°)ì˜ ê²½ìš°, encoder_model ì„ í†µí•œ ì˜ì–´ context vector (encoder_states) ê°€ decoder ëª¨ë¸ì˜ ì¸í’‹ìœ¼ë¡œ ë“¤ì–´ê°€ë©´ [decoder_outputs2] + decoder_states2 ë¥¼ output ìœ¼ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤. 

'I am happy' -> 'Bon jour joutemu' (ì €ëŠ” í”„ì•Œëª»ì´ì–´ì„œ ê·¸ëƒ¥ ì•„ë¬´ ì†Œë¦¬ë‚˜ í•´ë³´ì•˜ìŠµë‹ˆë‹¤) ì´ë¼ê³  í•˜ë©´ 'Bon'  + 'I am happy'  ì˜ ì •ë³´ë¥¼ ì••ì¶•í•´ì„œ ë‹´ê³  ìˆëŠ” context vector ì´ ëª¨ë¸ì— ë“¤ì–´ê°€ì„œ 'jour' + 'h(1),c(1)' ì´ë¼ëŠ” ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. 



ì´í›„ë¶€í„°ëŠ”

states_value = [h, c] ìœ¼ë¡œ ì €ì¥ë˜ê¸° ë•Œë¬¸ì— 'jour' + 'h(1),c(1)' ë¥¼ ë„£ì–´ì„œ 'joutemu +'h(2),c(2)' ë¥¼ ë°˜í™˜í•˜ê³  ... ì´ëŸ° ì‹ìœ¼ë¡œ ë¬¸ì¥ì˜ ëì¸ <eos> í† í°ì„ ë§Œë‚˜ê¸° ì „ê¹Œì§€ ë‹¨ì–´ë“¤ì„ (ê° ë‹¨ì–´ì— í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ ê°’)ì„ ì˜ ë±‰ì–´ë‚¼ ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤ ! 



ë˜

output_tokens, h, c = decoder_model.predict([target_seq] + states_value) ì¸ë°

â€‹        \# í˜„ì¬ ì‹œì ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë‹¤ìŒ ì‹œì ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì €ì¥        

target_seq = np.zeros((1,1))        

target_seq[0, 0] = sampled_token_index 

ìœ¼ë¡œ target_seq ê°’ì„ ì•ì„œì„œ tì‹œì ì—ì„œ ë°˜í™˜ëœ ê²°ê³¼ê°’ìœ¼ë¡œ ê³„ì† ê°±ì‹ í•˜ê¸° ë•Œë¬¸ì— í˜„ì¬ ì‹œì ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë‹¤ìŒ ì‹œì ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤. 

ì˜¤ëŠ˜ LSTM ê°œë…, LSTM nmt ëª¨ë¸ ì½”ë“œ , biLSTM + attention, transformer ê°œë… + open nmt íŠœí† ë¦¬ì–¼  ì´ë ‡ê²Œ ê³µë¶€í–ˆëŠ”ë° ëª»ë‹¤í•œ í¬ìŠ¤íŒ…ì€ ë‹¤ìŒì— ë˜ ì‹œê°„ ë‚  ë•Œ ì ìœ¼ë©´ì„œ ë³µìŠµí•´ë´ì•¼í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤ :)
