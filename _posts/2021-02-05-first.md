---
layout: post
title: 저는 드디어 graident descent 를 이해한 것 같습니다
subtitle: gradient descent 뒷북
tags: [gradient descent]
comment: True
---

## 미분의 개념이 미친듯이 햇갈렸던 지난 날들

학창시절에 미분은 문제 빨리 풀려고 개념 대충 봤다가 개념은 다 까먹고 공식으로 다 푸니까 (저는 문과여서 4점짜리 문제도 그렇게 개념을 딥하게 요구하진 않았던 걸로 ^^) 이후 딥러닝에서 미분 개념이 나올 때마다 미친듯이 햇갈리기 시작했습니다. 특히 머신러닝 첫 장에 나오는 gradient descent 부터 약간 아리송하는데 이해는 가니까 또 그냥 넘어가고 옙 뭐 그랬죠 🤫

## 미분 = 상대방 변화는 내 변화의 몇 배

#### gradient descent 를 직관적으로 이해해보았습니다. 저는 직관적인 것을 매우 좋아합니다

![스크린샷 2021-02-05 오후 10 35 08](https://user-images.githubusercontent.com/67775336/107040326-6ba30080-6802-11eb-9edf-a3cd034e327a.png)

L(w) 식에서 δL/δw 가 양수라면 L(W) 의 값은 증가한다고 할 수 있습니다. w를 위 그림의 직사각형의 밑변이라고 하면 w의 변화량은 δw 이고 상대(loss function) 의 변화량은 내 변화량의 δL/δw 배 이므로 전체 L(W)의 넓이가 이 빨간색 δw 와 연두색 δL/δw 를 곱한 것 만큼 늘어나게 되는 것입니다. 



그런데 gradient descent 공식은 W=W-a * δL/δw  이렇게 W 를 갱신해주는 것이므로 δw가 0.01 이라고 하고 δL/δw  이 0.04 라고 하면 이번엔 W(직사각형 그림에서 밑변) 의 길이를 아주 미세한 정도(δw ; δ=아주 극소량을 뜻한다) 도 아닌 그것보다 더 큰 나름 작지만 극소량은 아닌 δL/δw 만큼 줄여주게 되면 당연히 자연스럽게 넓이인 L(W)도 쪼그라들게 됩니다. 



이렇게 해서 가능할 때까지 (δL/δw 을 통해 weight 이 업데이트 될 때까지) 계속 L(W) 를 쪼그라들게하여서 minimum loss 가 나오게끔 weight update 을 해줍니다. 

![IMG_A003890C7818-1](https://user-images.githubusercontent.com/67775336/107039142-d4897900-6800-11eb-9705-da220807a330.jpeg)

여기서 제가 생각하는 포인트는 **δw < I δL/δw I** 이라는 것입니다. 실제 δ 자체가 눈에 보이지 않은 아주아주아~~~~~~~~~~~주 극 ~~~소량을 의미하므로 숫자로 나타낼 수도 없습니다. 당연히 아무리 작은 값이더라도 I δL/δw I 는 δw 보다 클 수밖에 없습니다. 

W 가 0 에서 시작하여서 δw 만큼 아주 조금조금씩 밑변의 길이가 길어져서 위와 같은 직사각형 L(W) 가 나온다고 하면 (=loss function 값이 점차 커진다고 하면) 바로 이전 조금 작은 값의 loss function 으로 되돌리려면 W - δw 만 해주면 될 것입니다. 그러면 loss function 이 아주 조금 작아지겠죠. 

하지만 애초에 δw 는 눈에 보이지 않은 아주아주 작은 변화량이므로 W - δw 를 해주기 보다는 W-a * δL/δw 를 해줍니다. 그러면 위 그림에서 밑변의 길이인 W가 0에서 δw 씩 아주 쬐끔쬐끔식 성장했는데 이번에는 쬐끔쬐금 다시 되돌려서 줄여주는게 아니라 a * δL/δw (learning rate 도 곱해줘가면서) 확확 밑변의 길이를 줄여주므로 loss function 도 가시적으로 눈에 띌만큼 조금씩 줄어들어갈 수가 있습니다. 

δL/δw 이 음수인 경우에는 똑같은 원리가 반대로 적용하여서 결국 gradient descent 를 통해 global minimum 에 도달할 수 있을 것입니다. 

물론 실제로 정확히 말하면 매개변수 중에 bias 도 있기 때문에 δL/δw 는 엄연히 w의 변화량이 loss function 에 끼치는 영향만을 고려한 편미분이고 저 직사각형 그림은 당연히 정확하지 않습니다. 그저 저의 직관적인 이해를 위해 사용된 그림입니다. 😬 

#### 

